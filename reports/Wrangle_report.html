<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">ol{margin:0;padding:0}table td,table th{padding:0}.c0{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c4{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:14pt;font-family:"Arial";font-style:normal}.c3{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c7{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:18pt;font-family:"Arial";font-style:normal}.c1{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c6{background-color:#ffffff;max-width:451.4pt;padding:72pt 72pt 72pt 72pt}.c5{font-size:14pt;font-weight:700}.c2{height:11pt}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c6"><p class="c1"><span class="c7">Wrangle Report </span></p><p class="c1"><span class="c3">By Gabriela Sikora</span></p><p class="c1 c2"><span class="c0"></span></p><p class="c1"><span class="c4">1. Introduction</span></p><p class="c1"><span class="c0">In this paper, I plan to briefly describe my wrangling efforts, which can be found in the Data Wrangling section of &lsquo;wrangle_act.ipynb&rsquo;. In particular, I will go into detail about the gathering, assessing and cleaning of data that I conducted.</span></p><p class="c1 c2"><span class="c0"></span></p><p class="c1 c2"><span class="c0"></span></p><p class="c1"><span class="c5">2. Gathering Data</span></p><p class="c1"><span class="c0">The data that was gathered was processed with 3 different methods.</span></p><p class="c1 c2"><span class="c0"></span></p><p class="c1"><span class="c3">2.1 &lsquo;enhanced_df&rsquo; </span></p><p class="c1"><span class="c0">The first involved a simple method of manually reading in a csv for the provided &#39;twitter-archive-enhanced.csv&rsquo;, which held information enhanced information about the WeRateDogs tweets. </span></p><p class="c1 c2"><span class="c0"></span></p><p class="c1"><span class="c3">2.2 &lsquo;prediction_df&rsquo; </span></p><p class="c1"><span class="c0">The second involved programmatically reading in a tsv that was created with a neural network to predict what dog breed a tweets image would hold. This was read in via URL with the Requests library and was then modified to become a csv. </span></p><p class="c1 c2"><span class="c0"></span></p><p class="c1"><span class="c3">2.3 &lsquo;tweet_df&rsquo; </span></p><p class="c1"><span class="c0">As for the third, this information was read in with help of Twitter&rsquo;s API. With the tweet_id&rsquo;s from the first dataframe, the tweets were checked if they were free of errors, and then the JSON data was added to &lsquo;tweet_json.txt&rsquo;, which was then read in to become the third dataframe.</span></p><p class="c1 c2"><span class="c0"></span></p><p class="c1 c2"><span class="c0"></span></p><p class="c1"><span class="c5">3. Assessing Data</span></p><p class="c1"><span>This section involved looking at the 3 </span><span>dataframes</span><span class="c0">&nbsp;that were just gathered. This section is broken down into 3 sections to reflect the 3 different dataframes. Each section begins with a general visual assessment as well as a look into the .info() to see more detailed information about the data. From then on, each section is assessed programmatically in order to better understand the details of their respective dataframes.</span></p><p class="c1 c2"><span class="c0"></span></p><p class="c1"><span class="c3">3.1 Identified Issues</span></p><p class="c1"><span class="c0">By means of assessing the dataframes, multiple quality and tidiness issues arose. The quality issues were related to any content related issues. Ultimately, 13 quality issues were addressed across the 3 dataframes, but naturally this number can easily continue to grow as there are many quality issues that can be found both visually and programmatically. As for tidiness issues, these are more structural in nature, and only 2 were found: the dog stages in the &lsquo;enhanced_df&rsquo; table were spread across 4 columns instead of being in one column, and all 3 dataframes could, in fact, be merged into one dataframe.</span></p><p class="c1 c2"><span class="c4"></span></p><p class="c1 c2"><span class="c4"></span></p><p class="c1"><span class="c4">4. Cleaning Data</span></p><p class="c1"><span class="c0">The process of cleaning the dataframes is guided by what was found from the quality and tidiness issues during the assessment of the dataframes. To begin, the dataframes were copied as to preserve the originals. Then, for each of the quality and tidiness issues identified in the previous section, a programmatic or manual cleaning of the issue occured. This was done in 3 steps: defining what should be done in pseudocode, applying the definition with code, and then testing to ensure that the expected results occurred.</span></p><p class="c1 c2"><span class="c0"></span></p><p class="c1"><span class="c0">Also, the result of cleaning the dataframes resulted in new issues, which was notable with the cleaning of tidiness issues. This, of course, resulted in an iterative process wherein some issues identified are the product of partially cleaned dataframe. </span></p><p class="c1 c2"><span class="c0"></span></p><p class="c1 c2"><span class="c4"></span></p><p class="c1"><span class="c5">5. Conclusion</span></p><p class="c1"><span class="c0">Overall, the data wrangling process is a heavily iterative process, wherein each step is crucial when aiming to create visualizations and come to conclusions. Since there are no set rules as to data creation, the steps between the data and the final clean dataframe make a world of difference, and the cleaner the dataframe, the more accurate the final conclusions will be. </span></p></body></html>